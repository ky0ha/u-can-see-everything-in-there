{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.9082e-39, 1.0194e-38, 9.1837e-39],\n",
      "        [8.4490e-39, 9.6429e-39, 8.4490e-39],\n",
      "        [9.6429e-39, 9.2755e-39, 1.0286e-38],\n",
      "        [9.0919e-39, 8.9082e-39, 9.2755e-39],\n",
      "        [8.4490e-39, 8.9082e-39, 9.1837e-39]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5381, 0.7355, 0.1815],\n",
      "        [0.1756, 0.3326, 0.2269],\n",
      "        [0.0625, 0.0760, 0.8611],\n",
      "        [0.4814, 0.0766, 0.1228],\n",
      "        [0.2985, 0.2194, 0.7939]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 1.1971, -0.3710, -0.3113],\n",
      "        [ 2.6235, -0.8904,  0.7508],\n",
      "        [-0.7718,  0.0355, -0.2700],\n",
      "        [-0.9942,  1.2708,  0.7141],\n",
      "        [ 0.2630,  0.3454, -0.0869]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_ones(5, 3, dtype=torch.float64)  # 返回的tensor默认具有相同的torch.dtype和torch.device\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float) # 指定新的数据类型\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())\n",
    "print(x.shape)\n",
    "# 返回的torch.Size其实就是一个tuple, 支持所有tuple的操作。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "还有很多函数可以创建Tensor，去翻翻官方API就知道了，下表给了一些常用的作参考。\n",
    "\n",
    "函数\t功能\n",
    "Tensor(*sizes)\t基础构造函数\n",
    "tensor(data,)\t类似np.array的构造函数\n",
    "ones(*sizes)\t全1Tensor\n",
    "zeros(*sizes)\t全0Tensor\n",
    "eye(*sizes)\t对角线为1，其他为0\n",
    "arange(s,e,step)\t从s到e，步长为step\n",
    "linspace(s,e,steps)\t从s到e，均匀切分成steps份\n",
    "rand/randn(*sizes)\t均匀/标准分布\n",
    "normal(mean,std)/uniform(from,to)\t正态分布/均匀分布\n",
    "randperm(m)\t随机排列\n",
    "这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6436,  0.5152,  0.0896],\n",
      "        [ 3.2911, -0.5042,  1.2812],\n",
      "        [-0.7459,  0.5220,  0.3735],\n",
      "        [-0.5883,  1.9447,  1.5651],\n",
      "        [ 0.7392,  1.1508,  0.8366]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6436,  0.5152,  0.0896],\n",
      "        [ 3.2911, -0.5042,  1.2812],\n",
      "        [-0.7459,  0.5220,  0.3735],\n",
      "        [-0.5883,  1.9447,  1.5651],\n",
      "        [ 0.7392,  1.1508,  0.8366]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6436,  0.5152,  0.0896],\n",
      "        [ 3.2911, -0.5042,  1.2812],\n",
      "        [-0.7459,  0.5220,  0.3735],\n",
      "        [-0.5883,  1.9447,  1.5651],\n",
      "        [ 0.7392,  1.1508,  0.8366]])\n"
     ]
    }
   ],
   "source": [
    "# 指定输出\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6436,  0.5152,  0.0896],\n",
      "        [ 3.2911, -0.5042,  1.2812],\n",
      "        [-0.7459,  0.5220,  0.3735],\n",
      "        [-0.5883,  1.9447,  1.5651],\n",
      "        [ 0.7392,  1.1508,  0.8366]])\n"
     ]
    }
   ],
   "source": [
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)\n",
    "# PyTorch操作inplace版本都有后缀_, 例如x.copy_(y), x.t_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1971, 0.6290, 0.6887])\n",
      "tensor([2.1971, 0.6290, 0.6887])\n"
     ]
    }
   ],
   "source": [
    "# 我们还可以使用类似NumPy的索引操作来访问Tensor的一部分，\n",
    "# 需要注意的是：索引出来的结果与原数据共享内存，也即修改一个，另一个会跟着修改。\n",
    "y = x[0, :]\n",
    "y += 1\n",
    "print(y)\n",
    "print(x[0, :]) # 源tensor也被改了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3]) torch.Size([15]) torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "print(x.size(), y.size(), z.size())\n",
    "# 注意view()返回的新Tensor与源Tensor虽然可能有不同的size，\n",
    "# 但是是共享data的，也即更改其中的一个，另外一个也会跟着改变。\n",
    "# (顾名思义，view仅仅是改变了对这个张量的观察角度，内部数据并未改变)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1971, -0.3710, -0.3113],\n",
      "        [ 1.6235, -1.8904, -0.2492],\n",
      "        [-1.7718, -0.9645, -1.2700],\n",
      "        [-1.9942,  0.2708, -0.2859],\n",
      "        [-0.7370, -0.6546, -1.0869]])\n",
      "tensor([ 2.1971,  0.6290,  0.6887,  2.6235, -0.8904,  0.7508, -0.7718,  0.0355,\n",
      "        -0.2700, -0.9942,  1.2708,  0.7141,  0.2630,  0.3454, -0.0869])\n"
     ]
    }
   ],
   "source": [
    "# 先用clone创造一个副本然后再使用view\n",
    "x_cp = x.clone().view(15)\n",
    "x -= 1\n",
    "print(x)\n",
    "print(x_cp)\n",
    "# 使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4701])\n",
      "-1.4701147079467773\n"
     ]
    }
   ],
   "source": [
    "# 另外一个常用的函数就是item(), 它可以将一个标量Tensor转换成一个Python number\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "由于x和y分别是1行2列和3行1列的矩阵，如果要计算x + y，那么x中第一行的2个元素被广播（复制）到了第二行和第三行，\n",
    "而y中第一列的3个元素被广播（复制）到了第二列。如此，就可以对2个3行2列的矩阵按元素相加。\n",
    "'''\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然view返回的Tensor与源Tensor是共享data的，但是依然是一个新的Tensor（因为Tensor除了包含data外还有一些其他属性），二者id（内存地址）并不一致\n",
    "\n",
    "我们很容易用numpy()和from_numpy()将Tensor和NumPy中的数组相互转换。但是需要注意的一点是： 这两个函数所产生的的Tensor和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！\n",
    "\n",
    "还有一个常用的将NumPy中的array转换成Tensor的方法就是torch.tensor(), 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的Tensor和原来的数据不再共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3]], device='cuda:0')\n",
      "tensor([[2., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 以下代码只有在PyTorch GPU版本上才会执行\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # GPU\n",
    "    y = torch.ones_like(x, device=device)  # 直接创建一个在GPU上的Tensor\n",
    "    x = x.to(device)                       # 等价于 .to(\"cuda\")\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # to()还可以同时更改数据类型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
