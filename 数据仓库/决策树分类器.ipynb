{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'细', '粗'}\n",
      "{'声音': {'细': '女'}}\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import operator\n",
    "\n",
    "def calcShannonEnt(dataSet):  # 计算数据的熵(entropy)\n",
    "    numEntries=len(dataSet)  # 数据条数\n",
    "    labelCounts={}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel=featVec[-1] # 每行数据的最后一个字（类别）\n",
    "        # ! 下列三行为过于繁琐的数据统计方法。dislike\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel]=0\n",
    "        labelCounts[currentLabel]+=1  # 统计有多少个类以及每个类的数量\n",
    "        # * 更加被喜欢的方法如下：\n",
    "        # * labelCounts[currentLabel] = labelCounts.setdefault(currentLabel, 0) + 1\n",
    "    shannonEnt=0    # shannonEnt 意思是 shannon entropy 香农熵\n",
    "    for key in labelCounts:\n",
    "        # * 熵是信息论中最重要和基础的概念，是概率分布的泛函，\n",
    "        # * 表示随机变量不确定性的大小。不确定性越大，熵越大。\n",
    "        # * 根据使用环境的不同、泛函的不同，熵分为：自信息、信息熵、联合熵、条件熵、相对熵、互信息\n",
    "        prob=float(labelCounts[key])/numEntries # 计算单个类的熵值\n",
    "        # * 信息量公式 $$ I(x)=-\\log_{2}{p(x)} $$\n",
    "        shannonEnt-=prob*log(prob,2) # 累加每个类的熵值\n",
    "    return shannonEnt\n",
    "\n",
    "# * createDataSet 方法创造了两个数据矩阵：\n",
    "# * 其中一个 dataSet 是数据集，是一个二维的数据。存储的内容是数据集内容\n",
    "# * 另一个 labels 是特征集，是一个一维向量数据。存储的内容是特征值\n",
    "# * createDataSet 通过 return 对上述两个数据矩阵进行了返回\n",
    "def createDataSet():    # 创造示例数据\n",
    "    dataSet = [['长', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['长', '细', '女'],\n",
    "               ['短', '细', '女'],\n",
    "               ['短', '粗', '女'],\n",
    "               ['长', '粗', '女'],\n",
    "               ['长', '粗', '女']]\n",
    "    labels = ['头发','声音']  #两个特征\n",
    "    return dataSet,labels\n",
    "\n",
    "# * splitDataSet：\n",
    "# @param :  dataSet  list[list]     数据集内容\n",
    "#           axis     int            某一列的下标\n",
    "#           value    any            想要从第 axis 列提取的内容的特征值\n",
    "# * 根据 axis 和 value 提取 dataSet 的第 axis 列内容为 value 的所有行的除了 axis 之外的数据（切分）\n",
    "# * 函数返回了一个数据矩阵，内容是 axis 为 value 的所有行的其他值的部分\n",
    "def splitDataSet(dataSet,axis,value): # 按某个特征分类后的数据\n",
    "    retDataSet=[]\n",
    "    # ? 也许可以使用 \n",
    "    # ? retDataSet = [(featVec[:axis] + featVec[axis+1:]) for featVec in dataSet if featVec[axis]==value]\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis]==value:\n",
    "            # ! 略加繁琐的列表切分方式，可以使用 featVec[:axis] + featVec[axis+1:]。dislike\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "# * 循环整个数据集的特征，有几个特征循环几次，循环过程中根据特征进行分类，对分类的数据求熵\n",
    "# * 熵和原始数据的熵之间的差值越大，证明对应的这个分类的特征越接近于最佳特征（或者就是最佳特征）\n",
    "# * 循环结束后返回最佳特征\n",
    "def chooseBestFeatureToSplit(dataSet):  # 选择最优的分类特征\n",
    "    numFeatures = len(dataSet[0])-1\n",
    "    baseEntropy = calcShannonEnt(dataSet)  # 原始的熵\n",
    "    bestInfoGain = 0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        # * 创建特征列表\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        # * 创建特征的集合\n",
    "        uniqueVals = set(featList)\n",
    "        newEntropy = 0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet,i,value)\n",
    "            prob =len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy +=prob*calcShannonEnt(subDataSet)  # 按特征分类后的熵\n",
    "        infoGain = baseEntropy - newEntropy  # 原始熵与按特征分类后的熵的差值\n",
    "        if (infoGain>bestInfoGain):   # 若按某特征划分后，熵值减少的最大，则次特征为最优分类特征\n",
    "            bestInfoGain=infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "def majorityCnt(classList):    #按分类后类别数量排序，比如：最后分类为2男1女，则判定为男；\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        # ! 下面三行依旧是不喜欢的词频统计写法\n",
    "        # * 以下判断类似于一个投票，根据决策树对其进行投票，最后根据投票结果判定预测结果\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def createTree(dataSet, labels):\n",
    "    # 把每个数据的类别存入一个单独的类别列表 classList\n",
    "    classList=[example[-1] for example in dataSet]  # 类别：男或女\n",
    "    # 特殊终止条件和递归终止条件\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "    # 查找最优分类特征\n",
    "    bestFeat=chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLabel=labels[bestFeat]\n",
    "    # 使用递归的方法创建决策树\n",
    "    myTree={bestFeatLabel:{}}\n",
    "    del(labels[bestFeat])\n",
    "    featValues=[example[bestFeat] for example in dataSet]\n",
    "    uniqueVals=set(featValues)\n",
    "    print(uniqueVals)\n",
    "    for value in uniqueVals:\n",
    "        subLabels=labels[:]\n",
    "        myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n",
    "        return myTree\n",
    "\n",
    "if __name__=='__main__':\n",
    "    dataSet, labels = createDataSet()\n",
    "    print(createTree(dataSet, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSet, labels = createDataSet()\n",
    "bestFeature = chooseBestFeatureToSplit(dataSet)\n",
    "t = splitDataSet(dataSet, bestFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'男'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = ['男', '男', '女', '男', '女']\n",
    "majorityCnt(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "579282893146967bc5d17141c601e8a8b8ad4a0a2a5a6fe1c87b1000077400d7"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
